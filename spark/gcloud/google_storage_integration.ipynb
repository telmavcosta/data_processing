{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IZVtSO5sqa8"
      },
      "source": [
        "## Reading/Writing data to Google Storage\n",
        "\n",
        "- Authenticate to Google\n",
        "- Install gcsfs and mount\n",
        "- Install gcsfuse and mount the bucket as a file system\n",
        "- Read data from bucket\n",
        "- Write data to bucket\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Authenticate to Google"
      ],
      "metadata": {
        "id": "HQa8AfAHDHqN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8fK3AR9Nier"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "project_id = 'data-eng-dev-437916'\n",
        "!gcloud config set project {project_id}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install gcsfs and pyspark"
      ],
      "metadata": {
        "id": "-Xl6MrCnDTZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gcsfs\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "tTIVU_O9XGi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install gcsfuse"
      ],
      "metadata": {
        "id": "hfIjGgl_DYXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "!apt -qq update\n",
        "!apt -qq install gcsfuse"
      ],
      "metadata": {
        "id": "038FQgCnADwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TS8Bwh3eDm1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Create local folder and mount the bucket as a file system"
      ],
      "metadata": {
        "id": "g2tke6JhDqDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir edit-data-eng-dev\n",
        "!gcsfuse edit-data-eng-dev edit-data-eng-dev"
      ],
      "metadata": {
        "id": "7mI20v0QAQVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Spark Session"
      ],
      "metadata": {
        "id": "HG33rv-JDuoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# .config(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
        "# .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
        "# .config(\"spark.hadoop.fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\") \\\n",
        "\n",
        "spark = (SparkSession.builder \\\n",
        "    .appName(\"ColabGCS\") \\\n",
        "    .getOrCreate())"
      ],
      "metadata": {
        "id": "tWxfnXcfXRVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\""
      ],
      "metadata": {
        "id": "UMTDEjXTXMu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# define paths\n",
        "bucket_name=\"edit-data-eng-dev\"\n",
        "lake_path=\"datalake/bronze\"\n",
        "table_path=\"basic_pays\"\n",
        "final_path=f\"gs://{bucket_name}/{lake_path}/{table_path}\"\n",
        "\n",
        "# since we're mounting the bucket as filesystem , the new path will be:\n",
        "# \"/content/edit-data-eng-dev/datalake/bronze/basic_pays\"\n",
        "# instead of\n",
        "# \"gs://edit-data-eng-dev/datalake/bronze/basic_pays\"\n"
      ],
      "metadata": {
        "id": "c0VHZajXWaAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read data from the bucket"
      ],
      "metadata": {
        "id": "vMuNoOS8Eia5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.parquet(\"/content/edit-data-eng-dev/datalake/bronze/basic_pays\")\n",
        "df.show()"
      ],
      "metadata": {
        "id": "15LOafBnV-gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write data to the bucket"
      ],
      "metadata": {
        "id": "VBJUodKEEqhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.format(\"parquet\").save(\"/content/edit-data-eng-dev/datalake/bronze/basic_pays_new\")"
      ],
      "metadata": {
        "id": "ZL98rmL4Bw39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Copying data through gsutils"
      ],
      "metadata": {
        "id": "LBUmN77sE1w_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil cp gs://edit-data-eng-dev/datalake/bronze/basic_pays/* gs://edit-data-eng-dev/datalake/bronze3/"
      ],
      "metadata": {
        "id": "CtRRE7Gga-aI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}