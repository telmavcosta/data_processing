{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/telmavcosta/data_processing/blob/main/spark_streaming/challenges/rep_final_challenges.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_GBE9UsyxwK"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9LeYFsPTjAb"
      },
      "source": [
        "# Setting up PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYXeODL0T1fO"
      },
      "outputs": [],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rcybt71kTDNt"
      },
      "source": [
        "# Context\n",
        "Message events are coming from platform message broker (kafka, pubsub, kinesis...).\n",
        "You need to process the data according to the requirements.\n",
        "\n",
        "Message schema:\n",
        "- timestamp\n",
        "- value\n",
        "- event_type\n",
        "- message_id\n",
        "- country_id\n",
        "- user_id\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkyPORKNSYvV"
      },
      "source": [
        "# Challenge 1\n",
        "\n",
        "Step 1\n",
        "- Change exising producer\n",
        "\t- Change parquet location to \"/content/lake/bronze/messages/data\"\n",
        "\t- Add checkpoint (/content/lake/bronze/messages/checkpoint)\n",
        "\t- Delete /content/lake/bronze/messages and reprocess data\n",
        "\t- For reprocessing, run the streaming for at least 1 minute, then stop it\n",
        "\n",
        "Step 2\n",
        "- Implement new stream job to read from messages in bronze layer and split result in two locations\n",
        "\t- \"messages_corrupted\"\n",
        "\t\t- logic: event_status is null, empty or equal to \"NONE\"\n",
        "    - extra logic: add country name by joining message with countries dataset\n",
        "\t\t- partition by \"date\" -extract it from timestamp\n",
        "\t\t- location: /content/lake/silver/messages_corrupted/data\n",
        "\n",
        "\t- \"messages\"\n",
        "\t\t- logic: not corrupted data\n",
        "\t\t- extra logic: add country name by joining message with countries dataset\n",
        "\t\t- partition by \"date\" -extract it from timestamp\n",
        "\t\t- location: /content/lake/silver/messages/data\n",
        "\n",
        "\t- technical requirements\n",
        "\t\t- add checkpint (choose location)\n",
        "\t\t- use StructSchema\n",
        "\t\t- Set trigger interval to 5 seconds\n",
        "\t\t- run streaming for at least 20 seconds, then stop it\n",
        "\n",
        "\t- alternatives\n",
        "\t\t- implementing single streaming job with foreach/- foreachBatch logic to write into two locations\n",
        "\t\t- implementing two streaming jobs, one for messages and another for messages_corrupted\n",
        "\t\t- (paying attention on the paths and checkpoints)\n",
        "\n",
        "\n",
        "  - Check results:\n",
        "    - results from messages in bronze layer should match with the sum of messages+messages_corrupted in the silver layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Udk3tohSaXOH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e0d4daa-e911-480f-c233-f31e378277cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading faker-37.4.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.11/dist-packages (from faker) (2025.2)\n",
            "Downloading faker-37.4.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faker\n",
            "Successfully installed faker-37.4.0\n"
          ]
        }
      ],
      "source": [
        "%pip install faker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDGMKwBdi1qy"
      },
      "source": [
        "# Producer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tPCOdivrfhYh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe49537e-623d-434b-e3da-885f0ada8a37"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import DataFrame\n",
        "from faker import Faker\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('Test streaming').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "fake = Faker()\n",
        "messages = [fake.uuid4() for _ in range(50)]\n",
        "\n",
        "def enrich_data(df, messages=messages):\n",
        "  fake = Faker()\n",
        "  new_columns = {\n",
        "      'event_type': F.lit(fake.random_element(elements=('OPEN', 'RECEIVED', 'SENT', 'CREATED', 'CLICKED', '', 'NONE'))),\n",
        "      'message_id': F.lit(fake.random_element(elements=messages)),\n",
        "      'channel': F.lit(fake.random_element(elements=('CHAT', 'EMAIL', 'SMS', 'PUSH', 'OTHER'))),\n",
        "      'country_id': F.lit(fake.random_int(min=2000, max=2015)),\n",
        "      'user_id': F.lit(fake.random_int(min=1000, max=1050)),\n",
        "  }\n",
        "  df = df.withColumns(new_columns)\n",
        "  return df\n",
        "\n",
        "def insert_messages(df: DataFrame, batch_id):\n",
        "  enrich = enrich_data(df)\n",
        "  enrich.write.mode(\"append\").format(\"parquet\").save(\"content/lake/bronze/messages/data\")\n",
        "\n",
        "# read stream\n",
        "df_stream = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 1).load()\n",
        "\n",
        "# write stream\n",
        "query = (df_stream.writeStream\n",
        ".option('checkpointLocation', 'content/lake/bronze/messages/checkpoint')\n",
        ".outputMode('append')\n",
        ".trigger(processingTime='1 seconds')\n",
        ".foreachBatch(insert_messages)\n",
        ".start()\n",
        ")\n",
        "\n",
        "query.awaitTermination(60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KNyUK3yplDhg"
      },
      "outputs": [],
      "source": [
        "query.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "ZWQExsnzlMFe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99781b82-8bdf-4164-dd1f-bbf6bd4139f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+----------+--------------------+-------+----------+-------+\n",
            "|           timestamp|value|event_type|          message_id|channel|country_id|user_id|\n",
            "+--------------------+-----+----------+--------------------+-------+----------+-------+\n",
            "|2025-07-05 15:25:...|    1|      OPEN|29342a46-49c6-48f...|   PUSH|      2007|   1028|\n",
            "|2025-07-05 15:25:...|    3|      OPEN|29342a46-49c6-48f...|   PUSH|      2007|   1028|\n",
            "|2025-07-05 15:25:...|    5|      OPEN|29342a46-49c6-48f...|   PUSH|      2007|   1028|\n",
            "|2025-07-05 15:25:...|    0|      OPEN|29342a46-49c6-48f...|   PUSH|      2007|   1028|\n",
            "|2025-07-05 15:25:...|    2|      OPEN|29342a46-49c6-48f...|   PUSH|      2007|   1028|\n",
            "|2025-07-05 15:25:...|    4|      OPEN|29342a46-49c6-48f...|   PUSH|      2007|   1028|\n",
            "|2025-07-05 15:25:...|   18|  RECEIVED|f34b9877-8c18-407...|  OTHER|      2014|   1002|\n",
            "|2025-07-05 15:26:...|   60|   CREATED|77c75e9d-32d1-482...|  EMAIL|      2014|   1015|\n",
            "|2025-07-05 15:25:...|   13|  RECEIVED|7ecbe2f2-6baf-4d0...|   CHAT|      2011|   1031|\n",
            "|2025-07-05 15:26:...|   50|   CREATED|31062dac-664a-445...|  EMAIL|      2010|   1044|\n",
            "|2025-07-05 15:25:...|   29|   CLICKED|2f4eaee2-f1ae-445...|  EMAIL|      2010|   1016|\n",
            "|2025-07-05 15:25:...|   37|   CLICKED|3f7bde71-b4a1-4d3...|  EMAIL|      2003|   1000|\n",
            "|2025-07-05 15:26:...|   49|   CLICKED|7b975da0-939e-467...|  OTHER|      2001|   1046|\n",
            "|2025-07-05 15:26:...|   52|   CLICKED|77c75e9d-32d1-482...|  OTHER|      2015|   1046|\n",
            "|2025-07-05 15:25:...|   41|   CREATED|8079657f-4798-48e...|  OTHER|      2003|   1028|\n",
            "|2025-07-05 15:25:...|    9|   CREATED|57185f5d-eb28-4c0...|  OTHER|      2010|   1018|\n",
            "|2025-07-05 15:26:...|   51|   CREATED|29c1bad1-9cb8-4d8...|  OTHER|      2005|   1036|\n",
            "|2025-07-05 15:25:...|   14|   CREATED|35d86b84-89a9-4d2...|   CHAT|      2009|   1049|\n",
            "|2025-07-05 15:26:...|   48|   CLICKED|29c1bad1-9cb8-4d8...|   PUSH|      2002|   1007|\n",
            "|2025-07-05 15:26:...|   47|   CREATED|77c75e9d-32d1-482...|   PUSH|      2004|   1039|\n",
            "+--------------------+-----+----------+--------------------+-------+----------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "63"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "df = spark.read.format(\"parquet\").load(\"content/lake/bronze/messages/data\")\n",
        "df.show()\n",
        "df.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RraxHCycMdEZ"
      },
      "source": [
        "# Additional datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cfsus3dxMcQI"
      },
      "outputs": [],
      "source": [
        "countries = [\n",
        "    {\"country_id\": 2000, \"country\": \"Brazil\"},\n",
        "    {\"country_id\": 2001, \"country\": \"Portugal\"},\n",
        "    {\"country_id\": 2002, \"country\": \"Spain\"},\n",
        "    {\"country_id\": 2003, \"country\": \"Germany\"},\n",
        "    {\"country_id\": 2004, \"country\": \"France\"},\n",
        "    {\"country_id\": 2005, \"country\": \"Italy\"},\n",
        "    {\"country_id\": 2006, \"country\": \"United Kingdom\"},\n",
        "    {\"country_id\": 2007, \"country\": \"United States\"},\n",
        "    {\"country_id\": 2008, \"country\": \"Canada\"},\n",
        "    {\"country_id\": 2009, \"country\": \"Australia\"},\n",
        "    {\"country_id\": 2010, \"country\": \"Japan\"},\n",
        "    {\"country_id\": 2011, \"country\": \"China\"},\n",
        "    {\"country_id\": 2012, \"country\": \"India\"},\n",
        "    {\"country_id\": 2013, \"country\": \"South Korea\"},\n",
        "    {\"country_id\": 2014, \"country\": \"Russia\"},\n",
        "    {\"country_id\": 2015, \"country\": \"Argentina\"}\n",
        "]\n",
        "\n",
        "countries = spark.createDataFrame(countries)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg2nx03_Sn62"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swvPj9hVpzNf"
      },
      "source": [
        "# Streaming Messages x Messages Corrupted"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf lake/silver/*\n",
        "!rm -rf \"{'\"content/lake/*"
      ],
      "metadata": {
        "id": "6ULldzJDOc8L"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "ZAHIZeZMlpoH",
        "outputId": "b527b740-42b5-48a2-fc78-24e975179ac0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "Schema must be specified when creating a streaming source DataFrame. If some files already exist in the directory, then depending on the file format you may be able to create a static DataFrame on that directory with 'spark.read.load(directory)' and infer schema from it.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-50-127814754.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# df_bronze = spark.readStream.format(\"parquet\").schema(message_schema).load(\"content/lake/bronze/messages/data\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mdf_bronze\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadStream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"content/lake/bronze/messages/data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0menriched_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_bronze\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"country_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"left\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"country\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"country\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/streaming/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    300\u001b[0m                     \u001b[0mmessage_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"arg_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"path\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"arg_value\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                 )\n\u001b[0;32m--> 302\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: Schema must be specified when creating a streaming source DataFrame. If some files already exist in the directory, then depending on the file format you may be able to create a static DataFrame on that directory with 'spark.read.load(directory)' and infer schema from it."
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "\n",
        "from pyspark.sql.types import *\n",
        "#from pyspark.sql.functions import col\n",
        "\n",
        "bronze_path=\"content/lake/bronze/messages/data\"\n",
        "silver_non_corrupted_path=\"content/lake/silver/messages/data\"\n",
        "silver_corrupted_path=\"content/lake/silver/messages_corrupted/data\"\n",
        "\n",
        "message_schema = StructType([StructField('timestamp', TimestampType(), True),\n",
        "                            StructField('value', LongType(), True),\n",
        "                            StructField('event_type', StringType(), True),\n",
        "                            StructField('message_id', StringType(), True),\n",
        "                            StructField('channel', StringType(), True),\n",
        "                            StructField('country_id', IntegerType(), True),\n",
        "                            StructField('user_id', IntegerType(), True)])\n",
        "\n",
        "df_bronze = spark.readStream.format(\"parquet\").schema(message_schema).load(\"content/lake/bronze/messages/data\")\n",
        "\n",
        "enriched_df = df_bronze.join(countries, on=\"country_id\", how=\"left\").withColumn(\"country\", F.col(\"country\")).withColumn(\"date\", F.to_date(F.col(\"timestamp\")))\n",
        "\n",
        "# Split into corrupted and valid\n",
        "is_corrupted = (F.col(\"event_type\").isNull() | (F.trim(F.col(\"event_type\")) == \"\") | (F.trim(F.col(\"event_type\")) == \"NONE\"))\n",
        "\n",
        "corrupted_messages_df = enriched_df.filter(is_corrupted)\n",
        "valid_messages_df = enriched_df.filter(~is_corrupted)\n",
        "\n",
        "\n",
        "corrupted_query = corrupted_messages_df.writeStream \\\n",
        "    .format(\"parquet\") \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .option(\"checkpointLocation\", \"content/lake/silver/messages_corrupted/checkpoint\") \\\n",
        "    .option(\"path\",\"content/lake/silver/messages_corrupted/data\") \\\n",
        "    .partitionBy(\"date\") \\\n",
        "    .trigger(processingTime=\"5 seconds\") \\\n",
        "    .start()\n",
        "\n",
        "valid_query = valid_messages_df.writeStream \\\n",
        "    .format(\"parquet\") \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .option(\"checkpointLocation\", \"content/lake/silver/messages/checkpoint\") \\\n",
        "    .option(\"path\",\"content/lake/silver/messages/data\") \\\n",
        "    .partitionBy(\"date\") \\\n",
        "    .trigger(processingTime=\"5 seconds\") \\\n",
        "    .start()\n",
        "\n",
        "corrupted_query.awaitTermination(20)\n",
        "valid_query.awaitTermination(20)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_query = df_bronze.writeStream \\\n",
        "    .format(\"parquet\") \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .option(\"checkpointLocation\", \"content/lake/silver/test/checkpoint\") \\\n",
        "    .option(\"path\",\"content/lake/silver/test/data\") \\\n",
        "    .trigger(processingTime=\"5 seconds\") \\\n",
        "    .start()\n",
        "\n",
        "test_query.awaitTermination(20)"
      ],
      "metadata": {
        "id": "a42GM6bsSyQi",
        "outputId": "9730bd7e-d5cb-4a4a-f9c0-5e89f70b41d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_query.stop()\n",
        "df = spark.read.format(\"parquet\").load(\"content/lake/silver/test/data\")\n",
        "df.show()\n",
        "df.count()"
      ],
      "metadata": {
        "id": "bYkpopiITKHL",
        "outputId": "914a3063-cfa5-4fd0-deb5-80251ca8bd14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+-----+----------+--------------------+-------+-------+-------------+----------+\n",
            "|country_id|           timestamp|value|event_type|          message_id|channel|user_id|      country|      date|\n",
            "+----------+--------------------+-----+----------+--------------------+-------+-------+-------------+----------+\n",
            "|      2007|2025-07-05 15:25:...|    1|      OPEN|29342a46-49c6-48f...|   PUSH|   1028|United States|2025-07-05|\n",
            "|      2007|2025-07-05 15:25:...|    3|      OPEN|29342a46-49c6-48f...|   PUSH|   1028|United States|2025-07-05|\n",
            "|      2007|2025-07-05 15:25:...|    5|      OPEN|29342a46-49c6-48f...|   PUSH|   1028|United States|2025-07-05|\n",
            "|      2007|2025-07-05 15:25:...|    0|      OPEN|29342a46-49c6-48f...|   PUSH|   1028|United States|2025-07-05|\n",
            "|      2007|2025-07-05 15:25:...|    2|      OPEN|29342a46-49c6-48f...|   PUSH|   1028|United States|2025-07-05|\n",
            "|      2007|2025-07-05 15:25:...|    4|      OPEN|29342a46-49c6-48f...|   PUSH|   1028|United States|2025-07-05|\n",
            "|      2009|2025-07-05 15:25:...|   14|   CREATED|35d86b84-89a9-4d2...|   CHAT|   1049|    Australia|2025-07-05|\n",
            "|      2014|2025-07-05 15:25:...|   18|  RECEIVED|f34b9877-8c18-407...|  OTHER|   1002|       Russia|2025-07-05|\n",
            "|      2009|2025-07-05 15:25:...|   17|   CLICKED|a373bb2b-2bf0-4e8...|    SMS|   1012|    Australia|2025-07-05|\n",
            "|      2015|2025-07-05 15:25:...|   11|      NONE|7b975da0-939e-467...|  OTHER|   1019|    Argentina|2025-07-05|\n",
            "|      2011|2025-07-05 15:25:...|   13|  RECEIVED|7ecbe2f2-6baf-4d0...|   CHAT|   1031|        China|2025-07-05|\n",
            "|      2010|2025-07-05 15:25:...|    9|   CREATED|57185f5d-eb28-4c0...|  OTHER|   1018|        Japan|2025-07-05|\n",
            "|      2014|2025-07-05 15:25:...|    8|      NONE|59e259fb-d9c1-4a6...|   PUSH|   1004|       Russia|2025-07-05|\n",
            "|      2011|2025-07-05 15:25:...|    6|      NONE|7b975da0-939e-467...|   CHAT|   1006|        China|2025-07-05|\n",
            "|      2002|2025-07-05 15:25:...|   10|      OPEN|a839ca9a-42ab-4f4...|    SMS|   1012|        Spain|2025-07-05|\n",
            "|      2000|2025-07-05 15:25:...|   15|          |26fd1e1d-1508-444...|  OTHER|   1042|       Brazil|2025-07-05|\n",
            "|      2000|2025-07-05 15:25:...|    7|          |57185f5d-eb28-4c0...|  EMAIL|   1026|       Brazil|2025-07-05|\n",
            "|      2012|2025-07-05 15:25:...|   12|          |e0b6b47d-5170-46b...|   PUSH|   1032|        India|2025-07-05|\n",
            "|      2011|2025-07-05 15:25:...|   16|          |52b25c6f-e2b3-4a5...|   PUSH|   1038|        China|2025-07-05|\n",
            "+----------+--------------------+-----+----------+--------------------+-------+-------+-------------+----------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corrupted_query.stop()\n",
        "valid_query.stop()"
      ],
      "metadata": {
        "id": "U_dnHdV1NbTa"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format(\"parquet\").load(\"content/lake/silver/messages_corrupted/data\")\n",
        "df.show()\n",
        "df.count()"
      ],
      "metadata": {
        "id": "YoR5YXvOQI1W",
        "outputId": "b98b6cd3-367f-4e74-d989-39ac644d4bc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+-----+----------+--------------------+-------+-------+---------+----------+\n",
            "|country_id|           timestamp|value|event_type|          message_id|channel|user_id|  country|      date|\n",
            "+----------+--------------------+-----+----------+--------------------+-------+-------+---------+----------+\n",
            "|      2015|2025-07-05 15:25:...|   11|      NONE|7b975da0-939e-467...|  OTHER|   1019|Argentina|2025-07-05|\n",
            "|      2014|2025-07-05 15:25:...|    8|      NONE|59e259fb-d9c1-4a6...|   PUSH|   1004|   Russia|2025-07-05|\n",
            "|      2011|2025-07-05 15:25:...|    6|      NONE|7b975da0-939e-467...|   CHAT|   1006|    China|2025-07-05|\n",
            "|      2000|2025-07-05 15:25:...|   15|          |26fd1e1d-1508-444...|  OTHER|   1042|   Brazil|2025-07-05|\n",
            "|      2000|2025-07-05 15:25:...|    7|          |57185f5d-eb28-4c0...|  EMAIL|   1026|   Brazil|2025-07-05|\n",
            "|      2012|2025-07-05 15:25:...|   12|          |e0b6b47d-5170-46b...|   PUSH|   1032|    India|2025-07-05|\n",
            "+----------+--------------------+-----+----------+--------------------+-------+-------+---------+----------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_valid = spark.read.format(\"parquet\").load(\"content/lake/silver/messages/data\")\n",
        "df_valid.show()\n",
        "df_valid.count()"
      ],
      "metadata": {
        "id": "P-I3SGC6Ri4a",
        "outputId": "7c4ce7d5-a26e-48d3-ab9f-966693bbf0f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+-----+----------+--------------------+-------+-------+-------------+----------+\n",
            "|country_id|           timestamp|value|event_type|          message_id|channel|user_id|      country|      date|\n",
            "+----------+--------------------+-----+----------+--------------------+-------+-------+-------------+----------+\n",
            "|      2007|2025-07-05 15:25:...|    1|      OPEN|29342a46-49c6-48f...|   PUSH|   1028|United States|2025-07-05|\n",
            "|      2007|2025-07-05 15:25:...|    3|      OPEN|29342a46-49c6-48f...|   PUSH|   1028|United States|2025-07-05|\n",
            "|      2007|2025-07-05 15:25:...|    5|      OPEN|29342a46-49c6-48f...|   PUSH|   1028|United States|2025-07-05|\n",
            "|      2007|2025-07-05 15:25:...|    0|      OPEN|29342a46-49c6-48f...|   PUSH|   1028|United States|2025-07-05|\n",
            "|      2007|2025-07-05 15:25:...|    2|      OPEN|29342a46-49c6-48f...|   PUSH|   1028|United States|2025-07-05|\n",
            "|      2007|2025-07-05 15:25:...|    4|      OPEN|29342a46-49c6-48f...|   PUSH|   1028|United States|2025-07-05|\n",
            "|      2010|2025-07-05 15:25:...|    9|   CREATED|57185f5d-eb28-4c0...|  OTHER|   1018|        Japan|2025-07-05|\n",
            "|      2002|2025-07-05 15:25:...|   10|      OPEN|a839ca9a-42ab-4f4...|    SMS|   1012|        Spain|2025-07-05|\n",
            "+----------+--------------------+-----+----------+--------------------+-------+-------+-------------+----------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLK9jpjCu3xE"
      },
      "source": [
        "## Checking data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nk8seEvbmvcU"
      },
      "outputs": [],
      "source": [
        "# TODO (count messages da silver (corrupted e not corrupted) = mensagens bronze)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfxIlBISSvRP"
      },
      "source": [
        "# Challenge 2\n",
        "\n",
        "- Run business report\n",
        "- But first, there is a bug in the system which is causing some duplicated messages, we need to exclude these lines from the report\n",
        "\n",
        "- removing duplicates logic:\n",
        "  - Identify possible duplicates on message_id, event_type and channel\n",
        "  - in case of duplicates, consider only the first message (occurrence by timestamp)\n",
        "  - Ex:\n",
        "    In table below, the correct message to consider is the second line\n",
        "\n",
        "```\n",
        "    message_id | channel | event_type | timestamp\n",
        "    123        | CHAT    | CREATED    | 10:10:01\n",
        "    123        | CHAT    | CREATED    | 07:56:45 (first occurrence)\n",
        "    123        | CHAT    | CREATED    | 08:13:33\n",
        "```\n",
        "\n",
        "- After cleaning the data we're able to create the busines report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RF9L9i25lk74"
      },
      "source": [
        "### Report 1\n",
        "  - Aggregate data by date, event_type and channel\n",
        "  - Count number of messages\n",
        "  - pivot event_type from rows into columns\n",
        "  - schema expected:\n",
        "  \n",
        "```\n",
        "|      date|channel|CLICKED|CREATED|OPEN|RECEIVED|SENT|\n",
        "+----------+-------+-------+-------+----+--------+----+\n",
        "|2024-12-03|    SMS|      4|      4|   1|       1|   5|\n",
        "|2024-12-03|   CHAT|      3|      7|   5|       8|   4|\n",
        "|2024-12-03|   PUSH|   NULL|      3|   4|       3|   4|\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPHSMSXnTKgu"
      },
      "outputs": [],
      "source": [
        "# report 1\n",
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxwOawo2lwQH"
      },
      "source": [
        "## Report 2\n",
        "\n",
        "- Identify the most active users by channel (sorted by number of iterations)\n",
        "- schema expected:\n",
        "\n",
        "```\n",
        "+-------+----------+----+-----+-----+----+---+\n",
        "|user_id|iterations|CHAT|EMAIL|OTHER|PUSH|SMS|\n",
        "+-------+----------+----+-----+-----+----+---+\n",
        "|   1022|         5|   2|    0|    1|   0|  2|\n",
        "|   1004|         4|   1|    1|    1|   1|  0|\n",
        "|   1013|         4|   0|    0|    2|   1|  1|\n",
        "|   1020|         4|   2|    0|    1|   1|  0|\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsS7bkAJmWsW"
      },
      "outputs": [],
      "source": [
        "# report 2\n",
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9_kzDbDwDOS"
      },
      "source": [
        "# Challenge 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ef0RjFTxwE5y"
      },
      "outputs": [],
      "source": [
        "# Theoretical question:\n",
        "\n",
        "# A new usecase requires the message data to be aggregate in near real time\n",
        "# They want to build a dashboard embedded in the platform website to analyze message data in low latency (few minutes)\n",
        "# This application will access directly the data aggregated by streaming process\n",
        "\n",
        "# Q1:\n",
        "- What would be your suggestion to achieve that using Spark Structure Streaming?\n",
        "Or would you choose a different data processing tool?\n",
        "\n",
        "# Q2:\n",
        "- Which storage would you use and why? (database?, data lake?, kafka?)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}