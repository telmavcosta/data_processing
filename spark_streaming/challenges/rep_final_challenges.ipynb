{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/telmavcosta/data_processing/blob/main/spark_streaming/challenges/rep_final_challenges.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_GBE9UsyxwK"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9LeYFsPTjAb"
      },
      "source": [
        "# Setting up PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uYXeODL0T1fO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78f5d966-43c4-4e59-ec4f-391fdb6941c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rcybt71kTDNt"
      },
      "source": [
        "# Context\n",
        "Message events are coming from platform message broker (kafka, pubsub, kinesis...).\n",
        "You need to process the data according to the requirements.\n",
        "\n",
        "Message schema:\n",
        "- timestamp\n",
        "- value\n",
        "- event_type\n",
        "- message_id\n",
        "- country_id\n",
        "- user_id\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkyPORKNSYvV"
      },
      "source": [
        "# Challenge 1\n",
        "\n",
        "Step 1\n",
        "- Change exising producer\n",
        "\t- Change parquet location to \"/content/lake/bronze/messages/data\"\n",
        "\t- Add checkpoint (/content/lake/bronze/messages/checkpoint)\n",
        "\t- Delete /content/lake/bronze/messages and reprocess data\n",
        "\t- For reprocessing, run the streaming for at least 1 minute, then stop it\n",
        "\n",
        "Step 2\n",
        "- Implement new stream job to read from messages in bronze layer and split result in two locations\n",
        "\t- \"messages_corrupted\"\n",
        "\t\t- logic: event_status is null, empty or equal to \"NONE\"\n",
        "    - extra logic: add country name by joining message with countries dataset\n",
        "\t\t- partition by \"date\" -extract it from timestamp\n",
        "\t\t- location: /content/lake/silver/messages_corrupted/data\n",
        "\n",
        "\t- \"messages\"\n",
        "\t\t- logic: not corrupted data\n",
        "\t\t- extra logic: add country name by joining message with countries dataset\n",
        "\t\t- partition by \"date\" -extract it from timestamp\n",
        "\t\t- location: /content/lake/silver/messages/data\n",
        "\n",
        "\t- technical requirements\n",
        "\t\t- add checkpint (choose location)\n",
        "\t\t- use StructSchema\n",
        "\t\t- Set trigger interval to 5 seconds\n",
        "\t\t- run streaming for at least 20 seconds, then stop it\n",
        "\n",
        "\t- alternatives\n",
        "\t\t- implementing single streaming job with foreach/- foreachBatch logic to write into two locations\n",
        "\t\t- implementing two streaming jobs, one for messages and another for messages_corrupted\n",
        "\t\t- (paying attention on the paths and checkpoints)\n",
        "\n",
        "\n",
        "  - Check results:\n",
        "    - results from messages in bronze layer should match with the sum of messages+messages_corrupted in the silver layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Udk3tohSaXOH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c98944ef-6502-4735-de44-e00ba51870a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading faker-37.4.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.11/dist-packages (from faker) (2025.2)\n",
            "Downloading faker-37.4.2-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faker\n",
            "Successfully installed faker-37.4.2\n"
          ]
        }
      ],
      "source": [
        "%pip install faker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDGMKwBdi1qy"
      },
      "source": [
        "# Producer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tPCOdivrfhYh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53d1490b-828f-47d1-f71f-58e7900789a1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import DataFrame\n",
        "from faker import Faker\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('Test streaming').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "fake = Faker()\n",
        "messages = [fake.uuid4() for _ in range(50)]\n",
        "\n",
        "def enrich_data(df, messages=messages):\n",
        "  fake = Faker()\n",
        "  new_columns = {\n",
        "      'event_type': F.lit(fake.random_element(elements=('OPEN', 'RECEIVED', 'SENT', 'CREATED', 'CLICKED', '', 'NONE'))),\n",
        "      'message_id': F.lit(fake.random_element(elements=messages)),\n",
        "      'channel': F.lit(fake.random_element(elements=('CHAT', 'EMAIL', 'SMS', 'PUSH', 'OTHER'))),\n",
        "      'country_id': F.lit(fake.random_int(min=2000, max=2015)),\n",
        "      'user_id': F.lit(fake.random_int(min=1000, max=1050)),\n",
        "  }\n",
        "  df = df.withColumns(new_columns)\n",
        "  return df\n",
        "\n",
        "def insert_messages(df: DataFrame, batch_id):\n",
        "  enrich = enrich_data(df)\n",
        "  enrich.write.mode(\"append\").format(\"parquet\").save(\"content/lake/bronze/messages/data\")\n",
        "\n",
        "# read stream\n",
        "df_stream = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 1).load()\n",
        "\n",
        "# write stream\n",
        "query = (df_stream.writeStream\n",
        ".option('checkpointLocation', 'content/lake/bronze/messages/checkpoint')\n",
        ".outputMode('append')\n",
        ".trigger(processingTime='1 seconds')\n",
        ".foreachBatch(insert_messages)\n",
        ".start()\n",
        ")\n",
        "\n",
        "query.awaitTermination(60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "KNyUK3yplDhg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c73cab1-21a8-4377-e95e-cca435987bbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:py4j.clientserver:There was an exception while executing the Python Proxy on the Python Side.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/py4j/clientserver.py\", line 617, in _call_proxy\n",
            "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pyspark/sql/utils.py\", line 120, in call\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pyspark/sql/utils.py\", line 117, in call\n",
            "    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)\n",
            "  File \"/tmp/ipython-input-3-2779494543.py\", line 26, in insert_messages\n",
            "    enrich.write.mode(\"append\").format(\"parquet\").save(\"content/lake/bronze/messages/data\")\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py\", line 1463, in save\n",
            "    self._jwrite.save(path)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
            "    return_value = get_return_value(\n",
            "                   ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
            "    return f(*a, **kw)\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/py4j/protocol.py\", line 326, in get_return_value\n",
            "    raise Py4JJavaError(\n",
            "py4j.protocol.Py4JJavaError: An error occurred while calling o4209.save.\n",
            ": java.io.InterruptedIOException: java.lang.InterruptedException\n",
            "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1010)\n",
            "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
            "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
            "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
            "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
            "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
            "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\n",
            "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\n",
            "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
            "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
            "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
            "\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\n",
            "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\n",
            "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
            "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
            "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
            "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
            "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
            "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
            "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
            "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
            "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\n",
            "\tat jdk.internal.reflect.GeneratedMethodAccessor58.invoke(Unknown Source)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
            "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
            "\tat com.sun.proxy.$Proxy30.call(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
            "Caused by: java.lang.InterruptedException\n",
            "\tat java.base/java.lang.Object.wait(Native Method)\n",
            "\tat java.base/java.lang.Object.wait(Object.java:328)\n",
            "\tat java.base/java.lang.ProcessImpl.waitFor(ProcessImpl.java:495)\n",
            "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1000)\n",
            "\t... 88 more\n",
            "\n"
          ]
        }
      ],
      "source": [
        "query.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ZWQExsnzlMFe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be32eeb1-71b5-4f2a-e6fd-d888a3d665ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+----------+--------------------+-------+----------+-------+\n",
            "|           timestamp|value|event_type|          message_id|channel|country_id|user_id|\n",
            "+--------------------+-----+----------+--------------------+-------+----------+-------+\n",
            "|2025-07-16 14:56:...|  107|      NONE|41f4d3e9-7f02-417...|  EMAIL|      2012|   1021|\n",
            "|2025-07-16 14:56:...|  109|      NONE|41f4d3e9-7f02-417...|  EMAIL|      2012|   1021|\n",
            "|2025-07-16 14:56:...|  111|      NONE|41f4d3e9-7f02-417...|  EMAIL|      2012|   1021|\n",
            "|2025-07-16 14:56:...|  113|      NONE|41f4d3e9-7f02-417...|  EMAIL|      2012|   1021|\n",
            "|2025-07-16 14:56:...|  115|      NONE|41f4d3e9-7f02-417...|  EMAIL|      2012|   1021|\n",
            "|2025-07-16 14:56:...|  117|      NONE|41f4d3e9-7f02-417...|  EMAIL|      2012|   1021|\n",
            "|2025-07-16 14:56:...|  119|      NONE|41f4d3e9-7f02-417...|  EMAIL|      2012|   1021|\n",
            "|2025-07-16 14:56:...|  108|      NONE|41f4d3e9-7f02-417...|  EMAIL|      2012|   1021|\n",
            "|2025-07-16 14:56:...|  110|      NONE|41f4d3e9-7f02-417...|  EMAIL|      2012|   1021|\n",
            "|2025-07-16 14:56:...|  112|      NONE|41f4d3e9-7f02-417...|  EMAIL|      2012|   1021|\n",
            "|2025-07-16 14:56:...|  114|      NONE|41f4d3e9-7f02-417...|  EMAIL|      2012|   1021|\n",
            "|2025-07-16 14:56:...|  116|      NONE|41f4d3e9-7f02-417...|  EMAIL|      2012|   1021|\n",
            "|2025-07-16 14:56:...|  118|      NONE|41f4d3e9-7f02-417...|  EMAIL|      2012|   1021|\n",
            "|2025-07-16 14:56:...|  120|      NONE|41f4d3e9-7f02-417...|  EMAIL|      2012|   1021|\n",
            "|2025-07-16 14:57:...|  157|   CREATED|914ccc77-9729-41b...|  EMAIL|      2009|   1023|\n",
            "|2025-07-16 14:57:...|  159|   CREATED|914ccc77-9729-41b...|  EMAIL|      2009|   1023|\n",
            "|2025-07-16 14:57:...|  161|   CREATED|914ccc77-9729-41b...|  EMAIL|      2009|   1023|\n",
            "|2025-07-16 14:57:...|  163|   CREATED|914ccc77-9729-41b...|  EMAIL|      2009|   1023|\n",
            "|2025-07-16 14:57:...|  158|   CREATED|914ccc77-9729-41b...|  EMAIL|      2009|   1023|\n",
            "|2025-07-16 14:57:...|  160|   CREATED|914ccc77-9729-41b...|  EMAIL|      2009|   1023|\n",
            "+--------------------+-----+----------+--------------------+-------+----------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "304"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "df = spark.read.format(\"parquet\").load(\"content/lake/bronze/messages/data\")\n",
        "df.show()\n",
        "df.count()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#query.isActive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFL8NtAS8v5B",
        "outputId": "7fda78c5-476f-449a-c6c9-566df097643e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RraxHCycMdEZ"
      },
      "source": [
        "# Additional datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cfsus3dxMcQI"
      },
      "outputs": [],
      "source": [
        "countries = [\n",
        "    {\"country_id\": 2000, \"country\": \"Brazil\"},\n",
        "    {\"country_id\": 2001, \"country\": \"Portugal\"},\n",
        "    {\"country_id\": 2002, \"country\": \"Spain\"},\n",
        "    {\"country_id\": 2003, \"country\": \"Germany\"},\n",
        "    {\"country_id\": 2004, \"country\": \"France\"},\n",
        "    {\"country_id\": 2005, \"country\": \"Italy\"},\n",
        "    {\"country_id\": 2006, \"country\": \"United Kingdom\"},\n",
        "    {\"country_id\": 2007, \"country\": \"United States\"},\n",
        "    {\"country_id\": 2008, \"country\": \"Canada\"},\n",
        "    {\"country_id\": 2009, \"country\": \"Australia\"},\n",
        "    {\"country_id\": 2010, \"country\": \"Japan\"},\n",
        "    {\"country_id\": 2011, \"country\": \"China\"},\n",
        "    {\"country_id\": 2012, \"country\": \"India\"},\n",
        "    {\"country_id\": 2013, \"country\": \"South Korea\"},\n",
        "    {\"country_id\": 2014, \"country\": \"Russia\"},\n",
        "    {\"country_id\": 2015, \"country\": \"Argentina\"}\n",
        "]\n",
        "\n",
        "countries = spark.createDataFrame(countries)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg2nx03_Sn62"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swvPj9hVpzNf"
      },
      "source": [
        "# Streaming Messages x Messages Corrupted"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf lake/silver/*\n",
        "!rm -rf \"{'\"content/lake/*"
      ],
      "metadata": {
        "id": "6ULldzJDOc8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZAHIZeZMlpoH",
        "outputId": "35fb5fdd-4cf3-4ad7-d604-d317853d6189",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# TODO\n",
        "\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "message_schema = StructType([StructField('timestamp', TimestampType(), True),\n",
        "                            StructField('value', LongType(), True),\n",
        "                            StructField('event_type', StringType(), True),\n",
        "                            StructField('message_id', StringType(), True),\n",
        "                            StructField('channel', StringType(), True),\n",
        "                            StructField('country_id', IntegerType(), True),\n",
        "                            StructField('user_id', IntegerType(), True)])\n",
        "\n",
        "df_bronze = spark.readStream.format(\"parquet\").schema(message_schema).load(\"content/lake/bronze/messages/data\")\n",
        "\n",
        "\n",
        "enriched_df = df_bronze.join(countries, on=\"country_id\", how=\"left\").withColumn(\"country\", F.col(\"country\")).withColumn(\"date\", F.to_date(F.col(\"timestamp\")))\n",
        "\n",
        "# Split into corrupted and valid\n",
        "is_corrupted = (F.col(\"event_type\").isNull() | (F.trim(F.col(\"event_type\")) == \"\") | (F.trim(F.col(\"event_type\")) == \"NONE\"))\n",
        "\n",
        "corrupted_messages_df = enriched_df.filter(is_corrupted)\n",
        "valid_messages_df = enriched_df.filter(~is_corrupted)\n",
        "\n",
        "\n",
        "corrupted_query = corrupted_messages_df.writeStream \\\n",
        "    .format(\"parquet\") \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .option(\"checkpointLocation\", \"content/lake/silver/messages_corrupted/checkpoint\") \\\n",
        "    .option(\"path\",\"content/lake/silver/messages_corrupted/data\") \\\n",
        "    .partitionBy(\"date\") \\\n",
        "    .trigger(processingTime=\"5 seconds\") \\\n",
        "    .start()\n",
        "\n",
        "valid_query = valid_messages_df.writeStream \\\n",
        "    .format(\"parquet\") \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .option(\"checkpointLocation\", \"content/lake/silver/messages/checkpoint\") \\\n",
        "    .option(\"path\",\"content/lake/silver/messages/data\") \\\n",
        "    .partitionBy(\"date\") \\\n",
        "    .trigger(processingTime=\"5 seconds\") \\\n",
        "    .start()\n",
        "\n",
        "corrupted_query.awaitTermination(20)\n",
        "valid_query.awaitTermination(20)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_corrupted = spark.read.format(\"parquet\").load(\"content/lake/silver/messages_corrupted/data\")\n",
        "df_corrupted.show()\n",
        "df_corrupted.count()"
      ],
      "metadata": {
        "id": "YoR5YXvOQI1W",
        "outputId": "00b53259-4cc2-4976-ee34-e7ebae051f9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+-----+----------+--------------------+-------+-------+---------+----------+\n",
            "|country_id|           timestamp|value|event_type|          message_id|channel|user_id|  country|      date|\n",
            "+----------+--------------------+-----+----------+--------------------+-------+-------+---------+----------+\n",
            "|      2009|2025-07-16 14:54:...|   24|      NONE|6490f409-e827-43a...|  EMAIL|   1009|Australia|2025-07-16|\n",
            "|      2009|2025-07-16 14:55:...|   45|      NONE|4da9c809-e448-495...|  EMAIL|   1006|Australia|2025-07-16|\n",
            "|      2009|2025-07-16 14:54:...|   23|      NONE|f4e1f692-8545-4be...|   PUSH|   1035|Australia|2025-07-16|\n",
            "|      2009|2025-07-16 14:54:...|   17|          |c75aaf6f-b8fb-496...|   CHAT|   1033|Australia|2025-07-16|\n",
            "|      2010|2025-07-16 14:55:...|   73|      NONE|c899f480-44c6-421...|  EMAIL|   1012|    Japan|2025-07-16|\n",
            "|      2010|2025-07-16 14:55:...|   77|      NONE|24240b8f-4ab6-4cb...|  OTHER|   1022|    Japan|2025-07-16|\n",
            "|      2012|2025-07-16 14:56:...|  107|      NONE|41f4d3e9-7f02-417...|  EMAIL|   1021|    India|2025-07-16|\n",
            "|      2012|2025-07-16 14:56:...|  109|      NONE|41f4d3e9-7f02-417...|  EMAIL|   1021|    India|2025-07-16|\n",
            "|      2012|2025-07-16 14:56:...|  111|      NONE|41f4d3e9-7f02-417...|  EMAIL|   1021|    India|2025-07-16|\n",
            "|      2012|2025-07-16 14:56:...|  113|      NONE|41f4d3e9-7f02-417...|  EMAIL|   1021|    India|2025-07-16|\n",
            "|      2012|2025-07-16 14:56:...|  115|      NONE|41f4d3e9-7f02-417...|  EMAIL|   1021|    India|2025-07-16|\n",
            "|      2012|2025-07-16 14:56:...|  117|      NONE|41f4d3e9-7f02-417...|  EMAIL|   1021|    India|2025-07-16|\n",
            "|      2012|2025-07-16 14:56:...|  119|      NONE|41f4d3e9-7f02-417...|  EMAIL|   1021|    India|2025-07-16|\n",
            "|      2012|2025-07-16 14:56:...|  108|      NONE|41f4d3e9-7f02-417...|  EMAIL|   1021|    India|2025-07-16|\n",
            "|      2012|2025-07-16 14:56:...|  110|      NONE|41f4d3e9-7f02-417...|  EMAIL|   1021|    India|2025-07-16|\n",
            "|      2012|2025-07-16 14:56:...|  112|      NONE|41f4d3e9-7f02-417...|  EMAIL|   1021|    India|2025-07-16|\n",
            "|      2012|2025-07-16 14:56:...|  114|      NONE|41f4d3e9-7f02-417...|  EMAIL|   1021|    India|2025-07-16|\n",
            "|      2012|2025-07-16 14:56:...|  116|      NONE|41f4d3e9-7f02-417...|  EMAIL|   1021|    India|2025-07-16|\n",
            "|      2012|2025-07-16 14:56:...|  118|      NONE|41f4d3e9-7f02-417...|  EMAIL|   1021|    India|2025-07-16|\n",
            "|      2012|2025-07-16 14:56:...|  120|      NONE|41f4d3e9-7f02-417...|  EMAIL|   1021|    India|2025-07-16|\n",
            "+----------+--------------------+-----+----------+--------------------+-------+-------+---------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "84"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_valid = spark.read.format(\"parquet\").load(\"content/lake/silver/messages/data\")\n",
        "df_valid.show()\n",
        "df_valid.count()"
      ],
      "metadata": {
        "id": "P-I3SGC6Ri4a",
        "outputId": "33ab13f1-76ef-4e93-fdf1-a896ceaa12ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+-----+----------+--------------------+-------+-------+-------------+----------+\n",
            "|country_id|           timestamp|value|event_type|          message_id|channel|user_id|      country|      date|\n",
            "+----------+--------------------+-----+----------+--------------------+-------+-------+-------------+----------+\n",
            "|      2000|2025-07-16 14:55:...|   79|   CREATED|41f4d3e9-7f02-417...|  EMAIL|   1011|       Brazil|2025-07-16|\n",
            "|      2000|2025-07-16 14:55:...|   57|      OPEN|9cf219d2-c0c0-476...|   CHAT|   1004|       Brazil|2025-07-16|\n",
            "|      2001|2025-07-16 14:55:...|   53|   CLICKED|407f8793-8033-47e...|  OTHER|   1026|     Portugal|2025-07-16|\n",
            "|      2001|2025-07-16 14:55:...|   36|   CREATED|ff8642f1-4aa6-4f9...|   CHAT|   1036|     Portugal|2025-07-16|\n",
            "|      2001|2025-07-16 14:56:...|   89|  RECEIVED|e49c1299-a224-408...|    SMS|   1049|     Portugal|2025-07-16|\n",
            "|      2001|2025-07-16 14:55:...|   68|   CREATED|2df8fff6-c758-43e...|    SMS|   1033|     Portugal|2025-07-16|\n",
            "|      2001|2025-07-16 14:56:...|   90|      SENT|9f0929f3-67ab-46f...|  OTHER|   1037|     Portugal|2025-07-16|\n",
            "|      2001|2025-07-16 14:54:...|   21|      SENT|62d3e3b3-4c5f-4e7...|    SMS|   1026|     Portugal|2025-07-16|\n",
            "|      2001|2025-07-16 14:55:...|   84|      SENT|9f0929f3-67ab-46f...|    SMS|   1033|     Portugal|2025-07-16|\n",
            "|      2009|2025-07-16 14:55:...|   31|   CLICKED|34b1e036-8493-493...|  EMAIL|   1028|    Australia|2025-07-16|\n",
            "|      2009|2025-07-16 14:56:...|   87|   CREATED|f4e1f692-8545-4be...|  EMAIL|   1009|    Australia|2025-07-16|\n",
            "|      2009|2025-07-16 14:55:...|   48|   CREATED|c2fe5129-8a8a-450...|    SMS|   1039|    Australia|2025-07-16|\n",
            "|      2009|2025-07-16 14:55:...|   39|      OPEN|65e0b2e1-8126-428...|  EMAIL|   1048|    Australia|2025-07-16|\n",
            "|      2010|2025-07-16 14:55:...|   67|  RECEIVED|ffbe5288-5d2d-4fa...|  OTHER|   1018|        Japan|2025-07-16|\n",
            "|      2010|2025-07-16 14:55:...|   38|   CREATED|566259b7-1fdb-483...|  EMAIL|   1041|        Japan|2025-07-16|\n",
            "|      2010|2025-07-16 14:55:...|   58|   CREATED|ff8642f1-4aa6-4f9...|   CHAT|   1029|        Japan|2025-07-16|\n",
            "|      2007|2025-07-16 14:56:...|   91|   CREATED|8072e114-7530-4c9...|  OTHER|   1005|United States|2025-07-16|\n",
            "|      2007|2025-07-16 14:55:...|   72|   CLICKED|1455cb7b-d016-463...|  OTHER|   1018|United States|2025-07-16|\n",
            "|      2007|2025-07-16 14:55:...|   35|   CLICKED|a971271d-8c11-449...|   CHAT|   1030|United States|2025-07-16|\n",
            "|      2007|2025-07-16 14:55:...|   66|  RECEIVED|94c5068e-ab1b-4f9...|    SMS|   1044|United States|2025-07-16|\n",
            "+----------+--------------------+-----+----------+--------------------+-------+-------+-------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "68"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corrupted_query.stop()\n",
        "valid_query.stop()"
      ],
      "metadata": {
        "id": "XnqG_8Hc9o-m"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLK9jpjCu3xE"
      },
      "source": [
        "## Checking data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "nk8seEvbmvcU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5faec297-120c-4a9f-f2ad-25b880e8995e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# TODO (count messages da silver (corrupted e not corrupted) = mensagens bronze)\n",
        "df = spark.read.format(\"parquet\").load(\"content/lake/bronze/messages/data\")\n",
        "df_corrupted = spark.read.format(\"parquet\").load(\"content/lake/silver/messages_corrupted/data\")\n",
        "df_valid = spark.read.format(\"parquet\").load(\"content/lake/silver/messages/data\")\n",
        "\n",
        "df_valid.count()\n",
        "df_corrupted.count()\n",
        "df.count()\n",
        "\n",
        "df_valid.count() + df_corrupted.count() == df.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfxIlBISSvRP"
      },
      "source": [
        "# Challenge 2\n",
        "\n",
        "- Run business report\n",
        "- But first, there is a bug in the system which is causing some duplicated messages, we need to exclude these lines from the report\n",
        "\n",
        "- removing duplicates logic:\n",
        "  - Identify possible duplicates on message_id, event_type and channel\n",
        "  - in case of duplicates, consider only the first message (occurrence by timestamp)\n",
        "  - Ex:\n",
        "    In table below, the correct message to consider is the second line\n",
        "\n",
        "```\n",
        "    message_id | channel | event_type | timestamp\n",
        "    123        | CHAT    | CREATED    | 10:10:01\n",
        "    123        | CHAT    | CREATED    | 07:56:45 (first occurrence)\n",
        "    123        | CHAT    | CREATED    | 08:13:33\n",
        "```\n",
        "\n",
        "- After cleaning the data we're able to create the busines report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "window = Window.partitionBy(\"message_id\", \"event_type\", \"channel\").orderBy(\"timestamp\")\n",
        "df_valid_with_rownum = df_valid.withColumn(\"row_num\", row_number().over(window))\n",
        "df_valid_deduped = df_valid_with_rownum.filter(col(\"row_num\") == 1).drop(\"row_num\")"
      ],
      "metadata": {
        "id": "2bFMh6K10pll"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RF9L9i25lk74"
      },
      "source": [
        "### Report 1\n",
        "  - Aggregate data by date, event_type and channel\n",
        "  - Count number of messages\n",
        "  - pivot event_type from rows into columns\n",
        "  - schema expected:\n",
        "  \n",
        "```\n",
        "|      date|channel|CLICKED|CREATED|OPEN|RECEIVED|SENT|\n",
        "+----------+-------+-------+-------+----+--------+----+\n",
        "|2024-12-03|    SMS|      4|      4|   1|       1|   5|\n",
        "|2024-12-03|   CHAT|      3|      7|   5|       8|   4|\n",
        "|2024-12-03|   PUSH|   NULL|      3|   4|       3|   4|\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "UPHSMSXnTKgu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42378e67-acd0-4c01-bbb7-efce6cd4780f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------+-------+-------+----+--------+----+\n",
            "|      date|channel|CLICKED|CREATED|OPEN|RECEIVED|SENT|\n",
            "+----------+-------+-------+-------+----+--------+----+\n",
            "|2025-07-16|  EMAIL|      3|      3|   2|       1|   2|\n",
            "|2025-07-16|    SMS|      4|      3|   2|       6|   5|\n",
            "|2025-07-16|   CHAT|      1|      4|   3|       2|   2|\n",
            "|2025-07-16|  OTHER|      2|      4|   1|       2|   2|\n",
            "|2025-07-16|   PUSH|      1|      3|   2|       1|   4|\n",
            "+----------+-------+-------+-------+----+--------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# report 1\n",
        "# TODO\n",
        "\n",
        "pivot_df = df_valid_deduped.groupBy(\"date\", \"channel\") \\\n",
        "    .pivot(\"event_type\") \\\n",
        "    .agg(count(\"*\"))\n",
        "\n",
        "pivot_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxwOawo2lwQH"
      },
      "source": [
        "## Report 2\n",
        "\n",
        "- Identify the most active users by channel (sorted by number of iterations)\n",
        "- schema expected:\n",
        "\n",
        "```\n",
        "+-------+----------+----+-----+-----+----+---+\n",
        "|user_id|iterations|CHAT|EMAIL|OTHER|PUSH|SMS|\n",
        "+-------+----------+----+-----+-----+----+---+\n",
        "|   1022|         5|   2|    0|    1|   0|  2|\n",
        "|   1004|         4|   1|    1|    1|   1|  0|\n",
        "|   1013|         4|   0|    0|    2|   1|  1|\n",
        "|   1020|         4|   2|    0|    1|   1|  0|\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "rsS7bkAJmWsW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "017017e8-42e9-45f1-bc01-ef3b63b887c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+-----+-----+----+---+----------+\n",
            "|user_id|CHAT|EMAIL|OTHER|PUSH|SMS|iterations|\n",
            "+-------+----+-----+-----+----+---+----------+\n",
            "|   1016|   0|    1|    0|   0|  0|         1|\n",
            "|   1005|   0|    0|    1|   0|  0|         1|\n",
            "|   1034|   0|    0|    0|   0|  1|         1|\n",
            "|   1030|   2|    1|    0|   0|  1|         4|\n",
            "|   1046|   0|    0|    0|   1|  1|         2|\n",
            "|   1008|   0|    0|    0|   0|  1|         1|\n",
            "|   1047|   0|    0|    0|   1|  0|         1|\n",
            "|   1021|   0|    0|    1|   1|  1|         3|\n",
            "|   1026|   0|    0|    1|   0|  1|         2|\n",
            "|   1028|   0|    1|    0|   0|  0|         1|\n",
            "|   1029|   0|    0|    0|   0|  1|         1|\n",
            "|   1032|   0|    1|    0|   0|  0|         1|\n",
            "|   1010|   0|    0|    0|   0|  1|         1|\n",
            "|   1050|   0|    0|    0|   0|  1|         1|\n",
            "|   1002|   0|    0|    1|   0|  1|         2|\n",
            "|   1048|   0|    1|    0|   0|  0|         1|\n",
            "|   1035|   0|    0|    0|   0|  1|         1|\n",
            "|   1017|   0|    0|    0|   1|  0|         1|\n",
            "|   1037|   0|    0|    1|   0|  0|         1|\n",
            "|   1036|   1|    0|    0|   0|  0|         1|\n",
            "+-------+----+-----+-----+----+---+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# report 2\n",
        "# TODO\n",
        "from pyspark.sql.functions import count, coalesce, lit, sum as spark_sum\n",
        "from functools import reduce\n",
        "\n",
        "pivot_df = df_valid_deduped.groupBy(\"user_id\") \\\n",
        "    .pivot(\"channel\") \\\n",
        "    .agg(count(\"*\"))\n",
        "\n",
        "# fill nulls with 0\n",
        "pivot_df = pivot_df.fillna(0)\n",
        "#pivot_df.show()\n",
        "\n",
        "# add iterarions column\n",
        "# Calculate total number of iterations across channels\n",
        "channel_cols = [c for c in pivot_df.columns if c != \"user_id\"]\n",
        "active_users_by_channel_df = pivot_df.withColumn(\"iterations\", reduce(lambda x, y: x + y, [col(c) for c in channel_cols]))\n",
        "#active_users_by_channel_df = pivot_df.withColumn(\"iterations\", sum(pivot_df[col] for col in pivot_df.columns if col != \"user_id\"))\n",
        "\n",
        "active_users_by_channel_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9_kzDbDwDOS"
      },
      "source": [
        "# Challenge 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ef0RjFTxwE5y"
      },
      "outputs": [],
      "source": [
        "# Theoretical question:\n",
        "\n",
        "# A new usecase requires the message data to be aggregate in near real time\n",
        "# They want to build a dashboard embedded in the platform website to analyze message data in low latency (few minutes)\n",
        "# This application will access directly the data aggregated by streaming process\n",
        "\n",
        "# Q1:\n",
        "- What would be your suggestion to achieve that using Spark Structure Streaming?\n",
        "Or would you choose a different data processing tool?\n",
        "\n",
        "# Q2:\n",
        "- Which storage would you use and why? (database?, data lake?, kafka?)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}