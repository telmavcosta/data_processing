{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/telmavcosta/data_processing/blob/main/spark_streaming/examples/example_2_rate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_GBE9UsyxwK"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOA_wQSmLd9z"
      },
      "source": [
        "# Usecase 2\n",
        "- Reading data from \"rate\"\n",
        "- Aggregating data by window time\n",
        "- Checking results from query in memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9LeYFsPTjAb"
      },
      "source": [
        "# Setting up PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYXeODL0T1fO",
        "outputId": "1ee183a2-7a54-454c-be9b-ce718d83edeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "637HFw00T3LP"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master('local').appName('Test streaming').getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write output in memory"
      ],
      "metadata": {
        "id": "NwzaZIoxqvrd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "I4mGPfB-Xg_C",
        "outputId": "06420f8e-cc80-4d25-dda7-a8a8aad41768",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "Cannot start query with name my_query as a query with that name is already active in this SparkSession",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-3455459645.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'my_query'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'complete'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/streaming/readwriter.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1525\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: Cannot start query with name my_query as a query with that name is already active in this SparkSession"
          ]
        }
      ],
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "# read stream\n",
        "stream1 = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 10).load()\n",
        "\n",
        "# transform\n",
        "transformed = stream1.withColumn(\"minute\", F.minute(\"timestamp\"))\n",
        "agg = transformed.groupBy(F.window(transformed.timestamp, \"5 seconds\")).count()\n",
        "\n",
        "# write stream in memory\n",
        "query = (agg.writeStream\n",
        ".format('memory')\n",
        ".queryName('my_query')\n",
        ".outputMode('complete')\n",
        ".start()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query.isActive"
      ],
      "metadata": {
        "id": "VcyLtc1bup5Z",
        "outputId": "5839b752-2531-4cf4-c4d7-10cede84116b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmLQLr1uX6w-",
        "outputId": "f33e8371-3b9b-4652-d10b-7e61c901ad24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------+-----+\n",
            "|window                                    |count|\n",
            "+------------------------------------------+-----+\n",
            "|{2025-07-05 08:36:40, 2025-07-05 08:36:45}|49   |\n",
            "|{2025-07-05 08:36:35, 2025-07-05 08:36:40}|50   |\n",
            "|{2025-07-05 08:36:30, 2025-07-05 08:36:35}|50   |\n",
            "|{2025-07-05 08:36:25, 2025-07-05 08:36:30}|50   |\n",
            "|{2025-07-05 08:36:20, 2025-07-05 08:36:25}|50   |\n",
            "|{2025-07-05 08:36:15, 2025-07-05 08:36:20}|50   |\n",
            "|{2025-07-05 08:36:10, 2025-07-05 08:36:15}|50   |\n",
            "|{2025-07-05 08:36:05, 2025-07-05 08:36:10}|50   |\n",
            "|{2025-07-05 08:36:00, 2025-07-05 08:36:05}|50   |\n",
            "|{2025-07-05 08:35:55, 2025-07-05 08:36:00}|50   |\n",
            "+------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"select * from my_query order by window desc\").show(10,False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TbLt4cUkX-JZ"
      },
      "outputs": [],
      "source": [
        "query.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write output as json"
      ],
      "metadata": {
        "id": "Dc3r3j-wj16K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf content/output"
      ],
      "metadata": {
        "id": "N_BZRFCCpGq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v180mzIciVZH"
      },
      "outputs": [],
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import DataFrame\n",
        "\n",
        "def save_parquet(df, batch_id):\n",
        "  (df\n",
        "   .withColumn(\"batch_id\",F.lit(batch_id))\n",
        "   .withColumn(\"load_time\",F.current_timestamp())\n",
        "   .write.mode(\"append\")\n",
        "   .parquet(\"content/output/rate_parquet\")\n",
        "  )\n",
        "\n",
        "# read stream\n",
        "stream1 = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 10).load()\n",
        "\n",
        "# transform\n",
        "transformed = stream1.withWatermark(\"timestamp\", \"5 seconds\").withColumn(\"minute\", F.minute(\"timestamp\"))\n",
        "agg = transformed.groupBy(F.window(transformed.timestamp, \"5 seconds\")).count()\n",
        "\n",
        "# write stream as parquet with foreachBatch (TC: customizar o codigo)\n",
        "query = (agg.writeStream\n",
        ".option('checkpointLocation', 'content/output/checkpoint')\n",
        ".trigger(processingTime='20 seconds')\n",
        ".outputMode('append')\n",
        ".foreachBatch(save_parquet)\n",
        ".start()\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = spark.read.format(\"parquet\").load(\"content/output/rate_parquet/\")\n",
        "result.sort(F.asc(\"window\")).show(100, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3SWIR6Ml8Al",
        "outputId": "e579771a-7fcc-409d-b938-0c0ce4d5fae9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------+-----+--------+--------------------------+\n",
            "|window                                    |count|batch_id|load_time                 |\n",
            "+------------------------------------------+-----+--------+--------------------------+\n",
            "|{2024-11-22 16:36:45, 2024-11-22 16:36:50}|17   |2       |2024-11-22 16:37:20.187321|\n",
            "|{2024-11-22 16:36:50, 2024-11-22 16:36:55}|50   |2       |2024-11-22 16:37:20.187321|\n",
            "|{2024-11-22 16:36:55, 2024-11-22 16:37:00}|50   |3       |2024-11-22 16:37:40.240769|\n",
            "|{2024-11-22 16:37:00, 2024-11-22 16:37:05}|50   |3       |2024-11-22 16:37:40.240769|\n",
            "|{2024-11-22 16:37:05, 2024-11-22 16:37:10}|50   |3       |2024-11-22 16:37:40.240769|\n",
            "|{2024-11-22 16:37:10, 2024-11-22 16:37:15}|50   |4       |2024-11-22 16:38:00.20902 |\n",
            "|{2024-11-22 16:37:15, 2024-11-22 16:37:20}|50   |4       |2024-11-22 16:38:00.20902 |\n",
            "|{2024-11-22 16:37:20, 2024-11-22 16:37:25}|50   |4       |2024-11-22 16:38:00.20902 |\n",
            "|{2024-11-22 16:37:25, 2024-11-22 16:37:30}|50   |4       |2024-11-22 16:38:00.20902 |\n",
            "|{2024-11-22 16:37:30, 2024-11-22 16:37:35}|50   |5       |2024-11-22 16:38:20.204491|\n",
            "|{2024-11-22 16:37:35, 2024-11-22 16:37:40}|50   |5       |2024-11-22 16:38:20.204491|\n",
            "|{2024-11-22 16:37:40, 2024-11-22 16:37:45}|50   |5       |2024-11-22 16:38:20.204491|\n",
            "|{2024-11-22 16:37:45, 2024-11-22 16:37:50}|50   |5       |2024-11-22 16:38:20.204491|\n",
            "+------------------------------------------+-----+--------+--------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query.stop()"
      ],
      "metadata": {
        "id": "DKB-MAPOoEre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enrich data with faker"
      ],
      "metadata": {
        "id": "GjAp1IKnvteX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faker"
      ],
      "metadata": {
        "id": "_gauZX8MFP5f",
        "outputId": "e6820854-14aa-4cfd-e2c6-37688b02999a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading Faker-33.0.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from faker) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
            "Downloading Faker-33.0.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faker\n",
            "Successfully installed faker-33.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf content/output/events"
      ],
      "metadata": {
        "id": "UFND4p5-2Na5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import DataFrame\n",
        "from faker import Faker\n",
        "\n",
        "def insert_into_table(df, batch_id):\n",
        "  fake = Faker()\n",
        "  new_columns = {\n",
        "      'name': F.lit(fake.name()),\n",
        "      'address': F.lit(fake.address()),\n",
        "      'email': F.lit(fake.email()),\n",
        "      'dob': F.lit(fake.date_of_birth()),\n",
        "      'phone': F.lit(fake.phone_number())\n",
        "  }\n",
        "  df = df.withColumns(new_columns)\n",
        "  df.write.mode(\"append\").format(\"parquet\").save(\"content/output/events\")\n",
        "\n",
        "# read stream\n",
        "df_stream = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 1).load()\n",
        "\n",
        "# write stream\n",
        "query = (df_stream.writeStream\n",
        ".outputMode('append')\n",
        ".trigger(processingTime='1 seconds')\n",
        ".foreachBatch(insert_into_table)\n",
        ".start()\n",
        ")"
      ],
      "metadata": {
        "id": "WCUhAzDOD4Q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query.stop()\n"
      ],
      "metadata": {
        "id": "KEMAlpKhwLNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.read.parquet(\"content/output/events\").show(100, False)"
      ],
      "metadata": {
        "id": "89s50dHjECqk",
        "outputId": "43972f9e-4221-44c9-e5a4-0de79fbff532",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------+-----+------------------+-----------------------------------------------------------+----------------------------+----------+---------------------+\n",
            "|timestamp              |value|name              |address                                                    |email                       |dob       |phone                |\n",
            "+-----------------------+-----+------------------+-----------------------------------------------------------+----------------------------+----------+---------------------+\n",
            "|2024-11-22 17:09:40.928|13   |Jennifer Padilla  |4723 Justin Stravenue Apt. 795\\nSouth Ashleyville, CT 67628|michaeltorres@example.com   |1916-02-22|+1-868-292-8492x48606|\n",
            "|2024-11-22 17:09:43.928|16   |Brandon Camacho   |88045 Owens Points Apt. 983\\nEast Michelleborough, GU 99123|torresedward@example.net    |1932-09-15|(772)402-8764        |\n",
            "|2024-11-22 17:09:29.928|2    |Timothy Lopez     |311 Church Throughway Suite 666\\nDominguezbury, MH 74128   |gloverlaura@example.net     |1934-03-16|(507)746-9168        |\n",
            "|2024-11-22 17:09:30.928|3    |Jordan Goodwin PhD|Unit 7962 Box 6328\\nDPO AA 80830                           |christopher48@example.net   |1950-08-14|5635485892           |\n",
            "|2024-11-22 17:09:31.928|4    |Jordan Goodwin PhD|Unit 7962 Box 6328\\nDPO AA 80830                           |christopher48@example.net   |1950-08-14|5635485892           |\n",
            "|2024-11-22 17:09:35.928|8    |Teresa Kim        |141 Travis Hills Suite 933\\nNorth Arielview, OK 62545      |morrisdylan@example.net     |2022-12-24|549-905-3641x42100   |\n",
            "|2024-11-22 17:09:28.928|1    |Terry Ortiz       |42897 Jessica Drive Suite 379\\nDiaztown, OH 16669          |christinebaldwin@example.org|1926-06-02|(934)859-1870        |\n",
            "|2024-11-22 17:09:36.928|9    |William Garrison  |4319 Karen Junction\\nMcdonaldside, NM 00564                |powersdawn@example.org      |1975-02-21|(518)344-7585x1578   |\n",
            "|2024-11-22 17:09:32.928|5    |Jennifer Lloyd    |503 Nicole Flats\\nNorth Dustin, GA 38015                   |baxterbrittany@example.com  |1944-04-21|(388)669-7867x77982  |\n",
            "|2024-11-22 17:09:38.928|11   |Brandon Bradley   |314 Williams Turnpike\\nSouth Kaylaton, CA 03974            |ryanjensen@example.com      |2019-05-18|311.569.1608         |\n",
            "|2024-11-22 17:09:27.928|0    |Tracy Davis       |62231 Courtney Ridge\\nSouth Sally, NV 88848                |jasonwolf@example.net       |1947-07-03|+1-889-207-5787x65305|\n",
            "|2024-11-22 17:09:42.928|15   |Lisa Wilson       |9984 Jonathan Glens Suite 773\\nRodgersside, AL 84653       |molly78@example.com         |2020-03-12|7418347065           |\n",
            "|2024-11-22 17:09:37.928|10   |Heather Sims      |0115 Miller Court\\nPort Williamville, MA 26583             |ejones@example.com          |1923-04-20|724-666-4026x647     |\n",
            "|2024-11-22 17:09:34.928|7    |Jessica Wilson    |274 Martin Harbor\\nLake Lisa, GA 35484                     |margaretrandall@example.org |1977-09-15|(866)986-0391        |\n",
            "|2024-11-22 17:09:39.928|12   |Lindsay Henry     |44338 Ashley Trail\\nLarryborough, SC 30519                 |piercetracy@example.org     |1952-03-03|862-338-3227         |\n",
            "|2024-11-22 17:09:41.928|14   |George Smith      |213 Tyler Cliffs\\nWest Amandafurt, PW 58380                |brian63@example.com         |1931-07-12|360.255.7480         |\n",
            "|2024-11-22 17:09:33.928|6    |Amy Hamilton      |PSC 7718, Box 3814\\nAPO AP 62469                           |rachel31@example.com        |1955-11-11|753.276.1547x296     |\n",
            "+-----------------------+-----+------------------+-----------------------------------------------------------+----------------------------+----------+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query.stop()"
      ],
      "metadata": {
        "id": "m22vpxcxIUNa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}